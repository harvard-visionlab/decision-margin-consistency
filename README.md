analysis draft: [![Open In Colab]([https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/?usp=sharing](https://colab.research.google.com/drive/1dQ3Z77PpAnjYbuTD36f9MY9wIq8OuKn_#scrollTo=MgY-a-W_P06r))

# Decision-margin consistency: a principled metric for human and machine performance alignment

Humans are noisy decision makers â€” they don't always provide the same exact response to the same exact stimulus, even when performing exactly the same task. This internal noise often dominates human performance to the extent that trial-by-trial analyses can yield low agreement between a person and themselves! Performance-concordance is an analysis method for measuring agreement between two decision making systems (e.g., humans vs. deep neural network models), assessing whether those systems have a tendency to make errors on the same inputs, while aggregating across stimulus repetitions or observers to cancel out internal noise and isolate stimulus-level agreement. Like error-consistency ([Geirhos et al., 2020](https://arxiv.org/abs/2006.16736)), this method enables a fine-grained analysis, assessing agreement at the level of individual sitmuli. The advantage of performance-concordance is that it can overcome stimulus-independent noise that can dominate human responses and mask high-levels of stimulus-level agreement. Re-analysis of existing datasets show that human-human agreement is greater than previously estimated, whereas human-machine agreement remains low, widening the gap between deep neural network models and human observers after taking internal noise into account. The paper is available on [arXiv]().


