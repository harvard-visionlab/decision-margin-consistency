{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fea8e-7175-41ea-96e7-7888669e1c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e704f-1d9e-4b84-a47e-50b2d6a05bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e14dfd-8fea-490b-a79d-2549ceae4a78",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ff696-1519-4e20-94b5-f2e7e5db100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from scipy.stats import pearsonr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc36780-2238-46d4-af1b-7b28cfe8b1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import decision_margin_consistency.analyses.self_consistency as analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982de16-e760-4812-aead-3e73e220de10",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb87b5-959b-4cd0-bf5f-825bc54b5a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = 'snr-edges-v1'\n",
    "df = analysis.load_data(exp_name, nTrials=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccd14f-6ed9-4c3e-af64-82cd86b94db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df.workerID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c04ecd-a99a-4a48-8538-c9fbb8e6838b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6795a-f2ab-417f-9b92-5f057e695613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(by=['workerID']).responseCorrect.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2eadd-03a6-4b8b-b22f-1268c5d8e19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# check for outliers\n",
    "\n",
    "Trim any subjects more than 3 STD from the mean (there were none in this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686b313-bf27-4619-80e5-ef02bcacf693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "subjects = df.workerID.unique()\n",
    "accuracy = df.groupby(by=['workerID']).responseCorrect.mean()\n",
    "M = accuracy.mean()\n",
    "STD = accuracy.std()\n",
    "lower = M - 3*STD \n",
    "upper = min(.99, M + 3*STD)\n",
    "outliers = (accuracy < lower) | (accuracy > upper)\n",
    "any(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d388340-ffa4-4d9c-bd18-70d81ced5ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.displot(accuracy)\n",
    "g.set(xlim=(.50, 1.00));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376dc56-6eff-42ae-87e5-a9dc29c1aa41",
   "metadata": {},
   "source": [
    "# self-consistency analysis\n",
    "\n",
    "We computed self-consistency by comparing subjects to themselves (1st vs. 2nd trial across images). TLDR, subjects do not respond consistency across trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479338d3-b4a8-4ac1-b661-35f9a5041bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b54cde-0025-4e55-bdbe-62ea16c7e2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = analysis.compute_summary(df)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c1640-6527-42bc-9f2a-7f695844f422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condNames = sorted(results.condName.unique())\n",
    "subjects = results.subject.unique()\n",
    "condNames, subjects, len(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0905a10-1b0d-4bca-9eef-6a4ec1aaf20d",
   "metadata": {},
   "source": [
    "For each individual, compute the mean accuracy for the 1st and 2nd trial, and the correlation across items for trial1 accuracy vs. trial2 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccd28b-e1bb-476f-af10-0d9115f9e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial1_acc = []\n",
    "trial2_acc = []\n",
    "corrs = []\n",
    "r2s = []\n",
    "N = len(subjects)\n",
    "for subject in subjects:\n",
    "    subset = results[results.subject==subject]\n",
    "    corr = pearsonr(subset.correct1, subset.correct2)[0]\n",
    "    r2 = corr**2\n",
    "    corrs.append(corr)\n",
    "    r2s.append(r2)\n",
    "    trial1_acc.append(subset.correct1.mean())\n",
    "    trial2_acc.append(subset.correct2.mean())\n",
    "avg_trial1_acc = np.mean(trial1_acc)\n",
    "avg_trial2_acc = np.mean(trial2_acc)\n",
    "avg_corr = np.mean(corrs)\n",
    "avg_r2 = np.mean(r2s)  \n",
    "print(f\"Summary of first vs. second response performance (N={N})\")\n",
    "print(f\"Mean proportion correct first trial = {avg_trial1_acc:3.3f}, vs. second trial = {avg_trial2_acc:3.3f}\")\n",
    "print(f\"Correlation across items first vs. second response, r={avg_corr:3.3f}, r2={avg_r2:3.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b1675-ff7b-4a9b-a157-2b01b17f4bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we compute cohen's kappa (error consistency) between first and second response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a04379-e403-48fa-9cbc-637e53899d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"\\nError consistency of first vs. second response (within a subject) ==>\")\n",
    "kappas = []\n",
    "corrs = []\n",
    "for subject in subjects:\n",
    "    subset = results[results.subject==subject]\n",
    "    condName = subset.iloc[0].condName\n",
    "    assert len(subset)==80\n",
    "    err_con = analysis.compute_error_consistency(subset.correct1.values, subset.correct2.values)\n",
    "    r = pearsonr(subset.correct1, subset.correct2)[0]\n",
    "    kappas.append(err_con['k'])\n",
    "    corrs.append(r)\n",
    "    # print(f\"{subject[0:5]}... ({condName}): c_exp={err_con['c_exp']:2.3f}, c_obs={err_con['c_obs']:2.3f} kappa={err_con['k']:2.3f}, r={r:2.3f}\")\n",
    "\n",
    "    \n",
    "# summary stats Kappa    \n",
    "kappas = np.array(kappas)\n",
    "mean_kappa = np.mean(kappas)\n",
    "sem_kappa = stats.sem(kappas)     \n",
    "ci_kappa = stats.t.interval(0.95, len(kappas) - 1, loc=mean_kappa, scale=sem_kappa)\n",
    "        \n",
    "print(f\"Cohen's kappa (average): {mean_kappa:3.2f} (95% CI: [{ci_kappa[0]:3.2f},{ci_kappa[1]:3.2f}])\")\n",
    "\n",
    "# summary stats correlations\n",
    "corrs = np.array(corrs)\n",
    "mean_corr = np.mean(corrs)\n",
    "sem_corr = stats.sem(corrs)     \n",
    "ci_corr = stats.t.interval(0.95, len(corrs) - 1, loc=mean_corr, scale=sem_corr)\n",
    "\n",
    "print(f\"Pearson's r (average): {mean_corr:3.2f} (95% CI: [{ci_corr[0]:3.2f},{ci_corr[1]:3.2f}]), r\\u00B2={mean_corr**2:3.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5ac99-de9f-4abf-bb86-71bd936d89a2",
   "metadata": {},
   "source": [
    "# Cohen's Kappa Scores are nearly Identical to Pearson R over response accuracy\n",
    "\n",
    "While Cohen's Kappa is well-justified for the binary correct/incorrect scores, the resulting kappa values are very similar to pearson r over the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a24e08-d0e9-4390-85a9-ae2a6234fdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats.sem(kappas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ef955-acec-48e3-a323-5ce7175215ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(kappas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762758-c91c-41f0-ba30-ed9f79d154f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=kappas, y=corrs)\n",
    "ax.axis('square');\n",
    "ax.set_xlim([.3,1.0]);\n",
    "ax.set_ylim([.3,1.0]);\n",
    "ax.set_xlabel(\"Cohen's Kappa\")\n",
    "ax.set_ylabel(\"pearson r\")\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c24fee-c627-42cd-abee-1c736cc25d14",
   "metadata": {},
   "source": [
    "# between-subject error consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624e747-6d40-47ad-954b-9691a3ecbe77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pdb import set_trace\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "\n",
    "def compute_between_subject_error_consistency(df):\n",
    "    corrs = []\n",
    "    corrs_half = []\n",
    "    corrs_12 = []\n",
    "    corrs_21 = []\n",
    "    r1s = []\n",
    "    r2s = []\n",
    "    \n",
    "    # Two groups of subjects; each group saw a different set of images, but saw each image twice\n",
    "    # so we can only compare responses within a group\n",
    "    subject_group = sorted(df.condName.unique())\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    mb = master_bar(subject_group)\n",
    "    for condName in mb:\n",
    "        df_ = df[df.condName==condName]\n",
    "        all_items = df_.item.unique()\n",
    "        subjects = df_.subject.unique()\n",
    "        num_subj = len(subjects)\n",
    "        for idx1 in progress_bar(range(0,num_subj-1), parent=mb):\n",
    "            sub1 = subjects[idx1]\n",
    "            dat1 = df_[df_.subject==sub1].reset_index()\n",
    "            assert len(dat1)==80\n",
    "            for idx2 in range(idx1+1,num_subj):\n",
    "                sub2 = subjects[idx2]      \n",
    "                dat2 = df_[df_.subject==sub2].reset_index()\n",
    "                assert len(dat1)==80\n",
    "                assert all((dat1.subject == dat2.subject)==False)\n",
    "                assert all((dat1.item == dat2.item)==True)\n",
    "                corr_S1Avg_S2Avg = pearsonr(dat1.correctAvg, dat2.correctAvg)[0]\n",
    "                corr_S1R1_S2R1 = pearsonr(dat1.correct1, dat2.correct1)[0]\n",
    "                corr_S1R2_S2R2 = pearsonr(dat1.correct2, dat2.correct2)[0]\n",
    "                corr_S1R1_S2R2 = pearsonr(dat1.correct1, dat2.correct2)[0]\n",
    "                corr_S1R2_S2R1 = pearsonr(dat1.correct2, dat2.correct1)[0]\n",
    "\n",
    "                errcon_S1R1_S2R1 = analysis.compute_error_consistency(dat1.correct1.values, dat2.correct1.values)['k']\n",
    "                errcon_S1R2_S2R2 = analysis.compute_error_consistency(dat1.correct2.values, dat2.correct2.values)['k']\n",
    "                errcon_S1R1_S2R2 = analysis.compute_error_consistency(dat1.correct1.values, dat2.correct2.values)['k']\n",
    "                errcon_S2R1_S2R1 = analysis.compute_error_consistency(dat1.correct2.values, dat2.correct1.values)['k']\n",
    "                \n",
    "                results['subj_group'].append(condName)\n",
    "                results['sub1'].append(sub1)\n",
    "                results['sub2'].append(sub2)\n",
    "                \n",
    "                results['corr_S1Avg_S2Avg'].append(corr_S1Avg_S2Avg)\n",
    "                results['corr_S1R1_S2R1'].append(corr_S1R1_S2R1)\n",
    "                results['corr_S1R2_S2R2'].append(corr_S1R2_S2R2)\n",
    "                results['corr_S1R1_S2R2'].append(corr_S1R1_S2R2)\n",
    "                results['corr_S1R2_S2R1'].append(corr_S1R2_S2R1)\n",
    "                results['corr_R1_R2_Avg'].append((corr_S1R1_S2R2+corr_S1R2_S2R1)/2)\n",
    "\n",
    "                results['errcon_S1R1_S2R1'].append(errcon_S1R1_S2R1)\n",
    "                results['errcon_S1R2_S2R2'].append(errcon_S1R2_S2R2)\n",
    "                results['errcon_S1R1_S2R2'].append(errcon_S1R1_S2R2)\n",
    "                results['errcon_S2R1_S2R1'].append(errcon_S2R1_S2R1)\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772a478-57b9-4a24-84ff-74c2b11a651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = analysis.compute_summary(df)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e820cd-6673-43c7-83da-31db15b4ca4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg = summary.groupby('item').mean(numeric_only=True).reset_index()\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c2e6d-d4f5-4e2f-95ff-eb0567a915d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sums = summary.groupby('item').sum(numeric_only=True).reset_index()\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3fc75-641e-4bb2-ad9f-4963812b3a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pearsonr(avg.correct1, avg.correct2)[0])\n",
    "sns.scatterplot(x=avg.correct1,y=avg.correct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e415e0-82b4-43d4-b2f5-a61ed074d07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "between = compute_between_subject_error_consistency(summary)\n",
    "between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86f70b-bae5-4732-9ba5-1c2e4cda142c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_summary_stats(df, startswith=['corr', 'errcon', 'dmc']):\n",
    "    columns = [name for name in df.columns.values if any([name.startswith(pattern) for pattern in startswith])]\n",
    "    results = defaultdict(list)\n",
    "    for col in columns:\n",
    "        scores = df[col].values\n",
    "        mean = np.mean(scores)\n",
    "        sem = stats.sem(scores)     \n",
    "        ci = stats.t.interval(0.95, len(scores) - 1, loc=mean, scale=sem)\n",
    "        \n",
    "        results['score'].append(col)\n",
    "        results['N'].append(len(scores))\n",
    "        results['mean'].append(mean)\n",
    "        results['sem'].append(sem)\n",
    "        results['ci_lower'].append(ci[0])\n",
    "        results['ci_upper'].append(ci[1])\n",
    "        \n",
    "        print(f\"{col}={mean:3.3f} (95% CI: [{ci[0]:3.3f},{ci[1]:3.3f}])\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13210f74-dfd2-4f1e-bd56-8b0ac7f2ecf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_summary_stats(between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00b74a-b02b-4d92-aa05-2739a51d3f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "between.errcon_S1R1_S2R1.mean(), between.errcon_S1R2_S2R2.mean(), between.errcon_S1R1_S2R2.mean(), between.errcon_S2R1_S2R1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02e661-85bb-4400-8b78-7b2d6f18374c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "between.corr_S1Avg_S2Avg.mean(), between.corr_S1R1_S2R1.mean(), between.corr_S1R2_S2R2.mean(), between.corr_R1_R2_Avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ede8f-0950-4f21-a57a-89096a446ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_group_avg_accuracy(df):\n",
    "    results = defaultdict(list)\n",
    "    for condName in condNames:\n",
    "        df_ = df[df.condName==condName]\n",
    "        all_items = natsorted(df_.item.unique())\n",
    "        subjects = df_.subject.unique()\n",
    "\n",
    "        for item in all_items:\n",
    "            subset = df_[df_.item==item]\n",
    "            assert len(subset)==len(subjects)\n",
    "            results['condName'].append(condName)\n",
    "            results['item'].append(item)\n",
    "            results['correct1'].append(subset.correct1.mean())\n",
    "            results['correct2'].append(subset.correct2.mean())\n",
    "            results['count1'].append(len(subset.correct1))\n",
    "            results['count2'].append(len(subset.correct2))\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "avg_acc = get_group_avg_accuracy(summary)\n",
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5609225-4486-4fe0-9958-95808c4883cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for condName in condNames:\n",
    "    subset = avg_acc[avg_acc.condName==condName]\n",
    "    assert len(subset)==80  \n",
    "    r = pearsonr(subset.correct1, subset.correct2)[0]\n",
    "    print(f\"{condName}: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")\n",
    "r = pearsonr(avg_acc.correct1, avg_acc.correct2)[0]    \n",
    "print(f\"overall: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d158f3-8600-418f-8338-ace3c487f353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "for condName in condNames:\n",
    "    subset = avg_acc[avg_acc.condName==condName]\n",
    "    assert len(subset)==80  \n",
    "    r = pearsonr(subset.correct1, subset.correct2)[0]\n",
    "    print(f\"{condName}: r={r:3.3f}\")\n",
    "\n",
    "    ax = sns.scatterplot(x=subset.correct1, y=subset.correct2)\n",
    "    ax.axis('square');\n",
    "    ax.set_xlim([0,1.2]);\n",
    "    ax.set_ylim([0,1.2]);\n",
    "    ax.set_xlabel(\"accuracy first presentation\")\n",
    "    ax.set_ylabel(\"accuracy second presentation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80615dac-9019-4ccc-8ae2-3f44c499760c",
   "metadata": {},
   "source": [
    "# dprime\n",
    "\n",
    "Question: Hey, we're using ideas from signal detection theory, why not calculate d'?\n",
    "\n",
    "Answer: OK, but then we have to \"adjust\" scores for floor/ceiling effects (d' is undefined for Pc=1.0 or 0.0). There are standard adjustments for that, but using these adjusted scores affects pearsonr, and so we should therefore use spearmanr for analyses using these adjusted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb6bc0-ce37-4460-869b-95d4789fa194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decision_margin_consistency.helpers.dprime import dprime_mAFC, adjusted_pc, adjusted_pc_edge_cases\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35714e-93c9-45e4-9092-578110edb1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_acc = get_group_avg_accuracy(summary)\n",
    "avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8107a-e725-4c33-817d-8ff4163233ea",
   "metadata": {},
   "source": [
    "Pearson's correlation between trial1 and trial2 is slightly degraded by converting to dprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216a73e-d26d-4612-942a-a7b21b618255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for condName in condNames:\n",
    "    subset = avg_acc[avg_acc.condName==condName]\n",
    "    assert len(subset)==80  \n",
    "    dprime1 = dprime_mAFC(subset.correct1, N=subset.count1, m=16)\n",
    "    dprime2 = dprime_mAFC(subset.correct2, N=subset.count2, m=16)\n",
    "    \n",
    "    r = pearsonr(dprime1, dprime2)[0]\n",
    "    print(f\"{condName}: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")\n",
    "dprime1 = dprime_mAFC(avg_acc.correct1, N=avg_acc.count1, m=16)\n",
    "dprime2 = dprime_mAFC(avg_acc.correct2, N=avg_acc.count2, m=16)    \n",
    "r = pearsonr(dprime1, dprime2)[0]    \n",
    "print(f\"overall: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b852a1-6db4-4887-b1bc-075d37e2655f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=dprime1, y=dprime2)\n",
    "ax.axis('square');\n",
    "ax.set_xlabel(\"dprime first presentation\")\n",
    "ax.set_ylabel(\"dprime second presentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb280c1-71a7-4dec-81f6-4b67ee00f67e",
   "metadata": {},
   "source": [
    "There's literally no difference in spearmanr between the dprime and original percent correct scores, so the only thing we buy is interpreting the scores as signal-to-noise measures of distance from the decision margin (aka snr signal strength or sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2392cf-7445-4bee-bf4a-2c4183da93ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for condName in condNames:\n",
    "    subset = avg_acc[avg_acc.condName==condName]\n",
    "    assert len(subset)==80  \n",
    "    dprime1 = dprime_mAFC(subset.correct1, N=subset.count1, m=16)\n",
    "    dprime2 = dprime_mAFC(subset.correct2, N=subset.count2, m=16)\n",
    "    \n",
    "    r = spearmanr(dprime1, dprime2)[0]\n",
    "    print(f\"{condName}: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")\n",
    "dprime1 = dprime_mAFC(avg_acc.correct1, N=avg_acc.count1, m=16)\n",
    "dprime2 = dprime_mAFC(avg_acc.correct2, N=avg_acc.count2, m=16)    \n",
    "r = spearmanr(dprime1, dprime2)[0]    \n",
    "print(f\"overall: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e67467-836f-4c36-83d8-263cb72562eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for condName in condNames:\n",
    "    subset = avg_acc[avg_acc.condName==condName]\n",
    "    assert len(subset)==80  \n",
    "    r = spearmanr(subset.correct1, subset.correct2)[0]\n",
    "    print(f\"{condName}: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")\n",
    "r = spearmanr(avg_acc.correct1, avg_acc.correct2)[0]    \n",
    "print(f\"overall: r={r:3.3f}, r\\u00B2={r*r:3.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411dfb8-df6b-4413-8aa6-2855eb2f7c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27c72d-d493-466e-ad0a-3b60c277d1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
